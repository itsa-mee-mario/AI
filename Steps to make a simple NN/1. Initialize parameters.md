[[inits_exp.ipynb]]


- It is important to pay attention to the initialization of weights to avoid diminishing or exploding gradients in the forward and backward pass of a neural net.
- To do this, we want to keep a zero-mean and unit variance. 
	- https://www.youtube.com/watch?v=s2coXdufOzE
- https://madaan.github.io/init/


# LSUV
The bigest problem in training neural nets is exploding and vanishing gradients, a lot of initialization methods exist.

LSUV stands for Layer Sequential Unit Variance

![[Pasted image 20220528170228.png]]
```python

def lsuv_module(m, xb):

	h = Hook(m, append_mean)

	if getattr(m, 'bias', None) is not None:

		while mdl(xb) is not None and abs(h.mean) > 1e-3:

			m.bias.data -= h.mean

	while mdl(xb) is not None and abs(h.std-1) > 1e-3:

		m.weight.data /= h.std
```


# Xavier Init
- sets mean to 0 and std dev to $\frac{1}{fan_{avg}}$
	- $fan_{avg} = \frac{fan_{in}+ fan_{out}}{2}$
	- $fan_{in}$ is the number of neurons in the previous layer, and $fan_{out}$ is the number of neurons in this layer

# He Init
- sets mean to 0 and std dev to $\frac{2}{fan_{in}}$
- used with ReLU and variants

# LeCun Init
- sets mean to 0 and std dev to $\frac{1}{fan_{in}}$
- used with SELU



[[2. do a matrix multiply (affine transform)]]

