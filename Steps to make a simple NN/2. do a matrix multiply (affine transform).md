# Matrix Multiplication
The fastest way to do this I know of is numpy's einsum. it is super fast and expressive, and uses C under the hood.

```python
def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)
```
[[einsum_explained.ipynb]]



# Knowledge Base
## Mat muls are just affine transforms

input:
![[Pasted image 20220418200936.png]]
output: 
![[Pasted image 20220418200945.png]]

![[Pasted image 20220418200919.png]]
> "this network basically takes the space fabric and performs a space transformation parametrised by several matrices and then by non-linearities."
> - https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-3/



---
# Code
## initial method (slow)
naive method, using 3 for loops
1. for each row in first matrix (i)
	1. for each column in second matrix (j)
		1. for each column in first matrix (k)
			element of product corresponding to row of first matrix and column of second matrix is given by the product of  $(i,k)^{th}$ element from the first matrix and $(k,j)^{th}$ element from the second matrix

```python
def matmul(a,b):
    ar,ac = a.shape # n_rows * n_cols
    br,bc = b.shape
    assert ac==br
    c = torch.zeros(ar, bc)
    for i in range(ar):
        for j in range(bc):
            for k in range(ac): # or br
                c[i,j] += a[i,k] * b[k,j]
    return c
```

## elementwise method

```python
def matmul(a,b):
    ar,ac = a.shape
    br,bc = b.shape
    assert ac==br
    c = torch.zeros(ar, bc)
    for i in range(ar):
        for j in range(bc):
            # Any trailing ",:" can be removed
            c[i,j] = (a[i,:] * b[:,j]).sum()
    return c
```

> each row of the first matrix is multiplied by a corresponding column of the second matrix. (like the view that Gilbert Strang shows in his lectures) (this is a correct was to **think** about it, but not the best computationaly)


## broadcasting
```python
def matmul(a,b):
    ar,ac = a.shape
    br,bc = b.shape
    assert ac==br
    c = torch.zeros(ar, bc)
    for i in range(ar):
        c[i]   = (a[i].unsqueeze(-1) * b).sum(dim=0)
    return c
```

Numpy has a thing called "broadcasting" which adjusts the shape of matrices in operations that dont make mathematical sense on their own to a form that does make sense.

 `.unsqueeze(-1)` adds a dimention to an array, changing shape from (x,y) to (x,y,1)
 

## einsum

```python
def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)
```
https://ajcr.net/Basic-guide-to-einsum/

[[einsum_explained.ipynb]]







[[3. do a non-linearity]]